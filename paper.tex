\documentclass[conference]{IEEEtran}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{url,hyperref,graphicx,float,times}
%\usepackage{sectsty}
%\usepackage{authblk}
\usepackage{textcomp}
\usepackage{cite}
% \graphicspath{{../pdf/}{../jpeg/}}
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage[caption=false,font=footnotesize]{subfig}

\usepackage{url}

\begin{document}

\title{Title}

\author{\IEEEauthorblockN{Ching-Chun (Jim) Huang} \IEEEauthorblockA{Department of\\Computer Science and Information
Engineering\\National Cheng Kung University, Taiwan\\ No.1, University Road, Tainan City 701, Taiwan (R.O.C.)\\ Email:
jserv@ccns.ncku.edu.tw} \and \IEEEauthorblockN{Chung-Fan Yang} \IEEEauthorblockA{Department of Electrical Engineering\\
National Cheng Kung University, Taiwan\\ No.1, University Road, Tainan City 701, Taiwan (R.O.C.)\\ Email:
E24026048@mail.ncku.edu.tw}}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Related Work}

\section{Methodology}

    Based on the works presented above, we have methods to measure the overall wake-up latency and context-switch
    latency, using cyclictest and lmbench. Also, it's able to show that the CFS algorithm has O(1) complexity on
    selecting next task, giving static latency. Thus, overall  and parts of latency of a Linux system can be determined,
    but the measurement methodology and tool of scheduler latency, shown in figure \ref{fig:latency_timeline}, are
    lacking. Therefore, we developed a evaluation tool of scheduler duration jitter, based on methodology method of A
    Decade of Wasted Cores \cite{Lozi:2016:LSD:2901318.2901326}. This methodology method enabled us to record the
    scheduler states including, executing process ID (PID), PID in scheduler run-queue, context switch points, timer
    interrupt execution, scheduling events, etc. on per processor core basis. This methodology method differs from
    profiling based methodology tools, e.g. perf \cite{perf}, lacking high fidelity, all-recorded profiling result of
    the target system. Our proposed tool can do full-detailed record in microsecond scale on x86-64 and ARM system,
    which is able to capture the impact to real-time performance of occasional short-burst scheduler events, which only
    last less that 10us. These events are mostly not shown in the result of sampling based measurement which focus on
    average-case execution time(ACET). By analyzing full-detailed record, we are able to not only capture the worst-case
    execution time (WCET), but also understand the cause of these incidents.

    In addition, the behavior and loading characteristic of typical work-load used during test in above works, e.g.
    hackbench, stress \cite{rt-tests}, etc. are far from the real-world real-time applications. These problems not only
    would reduce the significant of target system jitter measurement, but also will make incidents, harming the
    real-time performance, obscure. In result of this frustration, we also created a testing framework hosing an
    algorithm which resembled a real-time robot control system execution behavior, named mctest. This work-load
    application has the ability to be compiled either as user-space program or kernel module, provides observation of
    the characteristic of target application execution in both user-space and kernel-space. This would gives the insight
    of system-call and user-space I/O overhead, which does not exist in kernel module. In addition, mctest would do a
    series of identically executions and record the execution time of each turn. These records can be used to determined
    the execution jitter of the application on the real-time system, which besides wakeup latency. This could be used to
    reflect to the system-call overhead of the system. We have been utilizing mctest as the testing work-load and jitter
    measurement tool during methodology.

    \begin{figure} \centering \includegraphics[width=3.5in]{img/latency_timeline.png} \caption{Task Wake-up Latency}
    \label{fig:latency_timeline} \end{figure}

\section{Experiment}

    In aid of these developed tools and work presented above, we did multiple experiments and measurement of 2 ARM
    Cortex-A9 based platform, shown in table I. These platforms are selected as CMP based ARM system are emerging and
    being used in a wide field of applications. These systems are running buildroot \cite{buildroot} built Linux with
    kernel version 4.4 and PR\textunderscore RT \cite{preemptrt} patches.

    \begin{table*}[] \centering \caption{Experiment Platforms} \begin{tabular}{|l|c|c|} \hline Platform Name & Altera
        SoC FPGA Development Kit & Freescale i.mx6 Sabre \\ \hline Processor & 2x ARM Cortex-A9 & 4x ARM Cortex-A9 \\
        \hline Memory & 1 GB & 1 GB \\ \hline Additional configuration & None & \begin{tabular}[c]{@{}c@{}}L2 Cache
        locked down to CPU1,\\ Isolated CPU1,\\ Enabled tickless on CPU1 \\ Real-time task pinned to CPU1 \end{tabular}
    \\ \hline \end{tabular} \end{table*}

    Our experiment begins with known tuning practices which includes processor isolation \cite{isolatedcpu}, tickless
    feature of kernel \cite{tickless}, L2 cache lockdown. We compared the measurement result of mctest with and without
    these tuning techniques on Freesacle i.mx6 Sabre platform. Measurement results are shown in figure
    \ref{fig:imx6_mctest_v}, \ref{fig:imx6_mctest_iso}, and \ref{fig:imx6_mctest_lock}, verifying the effectiveness of
    these known tuning techniques and providing results as a baseline. We are able to see the WECT with and without
    tuning are 153us and 86us. This gives us the hint that the scheduling would harms the real-time performance. This
    scheduling overhead could be separated into 2 parts, scheduler overhead and cache contention on SMP system. With L2
    Cache lockdown technique we are able to mitigate most of the cache contention, but still resulting some degree of
    jitter about 76us in maximum.

    \begin{figure} \centering \includegraphics[width=3in]{img/mctest-none.png} \caption{Mctest on i.mx6 Sabre}
    \label{fig:imx6_mctest_v} \end{figure}
    
    \begin{figure} \centering \includegraphics[width=3in]{img/mctest-iso.png} \caption{Mctest on i.mx6 Sabre with
    isolated cpu and tickless feature} \label{fig:imx6_mctest_iso} \end{figure}
    
    \begin{figure} \centering \includegraphics[width=3in]{img/mctest-lock.png} \caption{Mctest on i.mx6 Sabre with
    isolated cpu, tickless feature and L2 cached lockdown} \label{fig:imx6_mctest_lock} \end{figure}
    
\subsection{System Call Overhead and Kernel Mode Linux}

    In addition, we compared the user-space mctest result with kernel space measurement, shown in figure
    \ref{fig:imx6_mctest_k_v} and \ref{fig:imx6_mctest_k_lock} with WECT of 27us and 17us. With these result, we are
    able to identify the sources of jitter are system calls. To mitigate these, we first proposed to rewrite real-time
    applications as kernel modules. But this is tedious and making the debugging process difficult. Thus, we proposed a
    second approach, Kernel Mode Linux(KML) \cite{KML} \cite{KMLConf}. KML enables the ability for specified user-space
    program running in kernel memory space. Thus, system calls become jump instructions, not instruction trapping the
    processor. Also, this would mitigate the cache contention while switching in and out the kernel during system call.
    In addition, we have also rewired system calls, which have vDSO implementations \cite{vDSO}, to use vDSO
    implementation via KML. The result of user-space program running in KML is shown in figure \ref{fig:imx6_mctest_kml}
    with WECT of 27us (excluding initialization jitter), which proving the effectiveness of jitter mitigation from KML.

    \begin{figure} \centering \includegraphics[width=3in]{img/mctest-k-none.png} \caption{Kernel mctest on i.mx6 Sabre}
    \label{fig:imx6_mctest_k_v} \end{figure}
    
    \begin{figure} \centering \includegraphics[width=3in]{img/mctest-k-lock.png} \caption{Kernel mctest on i.mx6 Sabre
    with isolated cpu, tickless feature and L2 cached lockdown} \label{fig:imx6_mctest_k_lock} \end{figure}
    
    \begin{figure} \centering \includegraphics[width=3in]{img/mctest-kml.png} \caption{Mctest on i.mx6 Sabre with
    isolated cpu, tickless feature in KML} \label{fig:imx6_mctest_kml} \end{figure}
    
\subsection{Scheduler Overhead Measurement}
    
    Identifying that scheduler is also a source of jitter, we applied our proposed scheduler profiling tool to evaluate
    and identify of which part of the scheduling process is the source of jitter. Knowing the IRQ latency, and
    context-switching latency, we would like to evaluate the scheduler duration. Traditionally, the latency of this part
    is considered static with the CFS algorithm complexity, designed to be O(1), but real measurement result are
    lacking. Thus, we set our proposed profiler to record the increment of scheduler run-queue and scheduler
    context-switching events with switched-to PID of each processors. A analyzer will process the recorded data and
    gives out the scheduler duration of a specified process, enabling us the understand the scheduling of real-time task
    in SMP system. The result of running a solo instance of cyclictest on SoC FPGA Development Kit is shown in figure
    \ref{fig:sd_noload}. The latency could be considered static with maximum 17us. This result matches traditional
    considerations, but with the interference from other work-load, the jitter will start to emerge.

    \begin{figure} \centering \includegraphics[width=3in]{img/sd-noload.png} \caption{Scheduler duration of solo
    cyclictest, no load} \label{fig:sd_noload} \end{figure}

\subsubsection{Scheduler Duration under Different Loading Condition}

    To measure the source of scheduler duration jitter, We created additional work-load, executing alone with
    cyclictest, schedulered with schedFIFO with highest priority in SoCFPGA based system, during profiling session. We
    created a overloaded condition, which has 8 stress threads, doing either CPU, memory or I/O bounded task. The
    scheduler duration of cyclictest real-time task under each type of loading condition is shown in figure
    \ref{fig:sd_cpu}, \ref{fig:sd_memory}, and \ref{fig:sd_io}. This result exhibits a mostly static scheduler
    duration with seldom jitter with maximum of 20us in CPU and I/O bounded loads. But putting the system in to heavy
    memory loading condition, the result, shown in figure \ref{fig:sd_memory} shows an increased average of scheduler
    duration and exhibits random jitter up to 34us. The spread of distribution of scheduler duration would also become
    wilder. This informs us that the scheduler duration is not a static value and will increase alone with the system's
    load, especially memory related load. This informs that the scheduler runnable task selection process execution time
    could be lengthen by user-space task, doing memory bounded tasks.

    \begin{figure} \centering \includegraphics[width=3in]{img/sd-cpu.png} \caption{Scheduler duration of cyclictest
    with CPU bounded load} \label{fig:sd_cpu} \end{figure}

    \begin{figure} \centering \includegraphics[width=3in]{img/sd-memory.png} \caption{Scheduler duration of cyclictest
    with memory bounded load} \label{fig:sd_memory} \end{figure}

    \begin{figure} \centering \includegraphics[width=3in]{img/sd-io.png} \caption{Scheduler duration distribution of
    cyclictest with I/O bounded load} \label{fig:sd_io} \end{figure}

    To mitigate the influence of memory bounded tasks, we isolated the real-time task into dedicated CPU core, and
    pinned memory bounded tasks to another CPU core. In addition, we make the shared L2 cache only accessible from the
    CPU core, running real-time task. The result of scheduler duration in shown in figure \ref{fig:sd_iso}. The
    scheduler duration is stabilized and showing as distribution as other loading conditions. The stress memory bounded
    tasks were designed to spin on malloc and free calls. This suggests us that memory allocation and deallocation would
    decrease the scheduler performance and increase its execution jitter via cache and memory model. Also, we gathered
    the result of pining both real-time and loads to same CPU core. Figure \ref{fig:sd_same_no_cache} shows the result
    of running on CPU core without L2 cache.  Comparing with figure \ref{fig:sd_memory}, result under 2 core SMP, this
    result showns a slight improvement of average latency and jitter. This informs that shared L2 cache and dynamic
    number of loading tasks, sharing CPU with real-time task, would be the source of scheduler duration jitter. Thus,
    decrease the use of dynamic memory allocation, real-time task CPU isolation and cache lockdown could prevent raising
    the scheduler duration, keeping it as a stable value, about 10us.

    \begin{figure} \centering \includegraphics[width=3in]{img/sd-iso.png} \caption{Scheduler duration of cyclictest with
    memory bounded load on different cores} \label{fig:sd_iso} \end{figure}

    \begin{figure} \centering \includegraphics[width=3in]{img/sd-same-no-cache.png} \caption{Scheduler duration of
    cyclictest with memory bounded load on same core} \label{fig:sd_same_no_cache} \end{figure}

\section{Conclusion}

    We have evaluated the real-time behavior of Linux by profiling kernel scheduler and measuring the latency of various
    kernel variants. An intensive interrupt load can cause long OS latencies due to the design of the interrupt
    processing mechanism. We proposed new tools to visualize task scheduling in fine-grained scale(microsecond level).
    This enabled us not only focusing on interrupt latency, but also scheduler durations, lock, and etc. It would thus
    be highly desirable to combine existing techniques, e.g KML, isolated CPU, tickless kernel, to improve task
    responsiveness under various target application characteristics, on top of PREEMPT\textunderscore RT.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
